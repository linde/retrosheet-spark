{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from os.path import basename, splitext\n",
    "import re\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# TODO figure out why when scanning all records grep got a count of\n",
    "# 120924 but the data frame has 120700.\n",
    "# TODO make a SQL comment for dataframes for defaulted ints (like -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retroFilePath = \"retrosheet-data/*/*.EV[AN]\"\n",
    "## retroFilePath = \"retrosheet-data/2010*/*.EV[AN]\" # just the 10s\n",
    "\n",
    "rosterPath = \"retrosheet-data/*/*.ROS\"\n",
    "teamPath = \"retrosheet-data/*/TEAM[0-9][0-9][0-9][0-9]\"\n",
    "\n",
    "DIST_PARQUET_DIR = \"dist/parquet/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EVENT FILES: the following is generic stuff to processes\n",
    "# the event files, ie *.EV[AN]\n",
    "\n",
    "\n",
    "# proc to process a whole game's events. \n",
    "# by using '\\nid,' as our delim above, we get called with a whole game's\n",
    "# events. we resplit them on newlines so lines can be parsed with event type\n",
    "# specific parsers, but first prepend the game_id and a sequence into the game's\n",
    "# events to each record.\n",
    "def processRecord( rec ):\n",
    "    \n",
    "    (k,recordsBetweenIdLines) = rec;\n",
    "\n",
    "    # first, resplit the events\n",
    "    events = recordsBetweenIdLines.splitlines()\n",
    "\n",
    "    # first off, disgard non-game related records, ie they dont appear \n",
    "    # after an \"id,\" token. examples:\n",
    "    # \n",
    "    # com,\"Copyright 2001 by Stats Inc.\"\n",
    "    # com,\"All Rights Reserved.\"\n",
    "    if (events[0].startswith(\"com,\")):\n",
    "        return []\n",
    "    \n",
    "    # if we're not the first record, we dont still have the \"id,\" token\n",
    "    # restore the record so the id record parser can work like the others\n",
    "    \n",
    "    if (events[0].startswith(\"id,\") is False):\n",
    "        events[0] = \"id,\" + events[0]\n",
    "        \n",
    "    # now, get the game_id that will be prepended to each record\n",
    "    game_id = events[0].split(\",\")[1]    \n",
    "    homeTeamCode = game_id[:3]\n",
    "    gameYear = int(game_id[3:7])\n",
    "    gameMonth = int(game_id[7:9])\n",
    "    gameDay = int(game_id[9:11])\n",
    "    gameDate = date(gameYear, gameMonth, gameDay)\n",
    "\n",
    "    # now emit lines prepended with game_id and the seq into the game, then\n",
    "    # the original lines from the file.\n",
    "    return (( \n",
    "        game_id, \n",
    "        seq,\n",
    "        homeTeamCode,\n",
    "        gameDate,\n",
    "        record.split(\",\") ## TODO consider flattening this in\n",
    "    ) for seq, record in enumerate(events))\n",
    "\n",
    "## some stuff to hide details of the global columns prepended \n",
    "## to each event in processRecord() so the mappers below dont\n",
    "## need to know about them\n",
    "\n",
    "# TODO this should probably be a func, not a global\n",
    "BASE_GLOBAL_COLUMNS_SCHEMA = [    \n",
    "    StructField(\"game_id\",StringType(),False), \n",
    "    StructField(\"seq\",ShortType(),False), # TODO should be short\n",
    "    StructField(\"homeTeamCode\",StringType(),False), \n",
    "    StructField(\"gameDate\",DateType(),False), \n",
    "]\n",
    "def getBaseColumns(record):\n",
    "    return [\n",
    "        record[0],\n",
    "        int(record[1]),\n",
    "        record[2],\n",
    "        record[3]\n",
    "    ]\n",
    "def getFields(record): \n",
    "    return record[-1]\n",
    "\n",
    "\n",
    "#### Here is where the processing game event processing begins\n",
    "\n",
    "# first, make an RDD that has all of a games records combined\n",
    "\n",
    "retrosheet = sc.newAPIHadoopFile( \n",
    "    retroFilePath, \n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat', \n",
    "    'org.apache.hadoop.io.LongWritable', \n",
    "    'org.apache.hadoop.io.Text', \n",
    "    conf={\n",
    "        'textinputformat.record.delimiter':'\\nid,'\n",
    "    }\n",
    ")\n",
    "\n",
    "# then, do a flatmap to get the events between \"id\" records and to \n",
    "# process them using the proc above\n",
    "combinedFlattenedEvents = retrosheet.flatMap(processRecord)\n",
    "combinedFlattenedEvents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## EVENT TYPE SPECIFIC STUFF\n",
    "\n",
    "\n",
    "# a record mapper for key, [value] types such as \"info\", etc\n",
    "def multiValueRecordMapper(record):\n",
    "    retArray = getBaseColumns(record)       # base fields\n",
    "    retArray.append(getFields(record)[1])   # the key column\n",
    "    retArray.append(getFields(record)[2:])  # the array of values\n",
    "    return retArray\n",
    "\n",
    "# a util to provide fields for the dataframe schemas\n",
    "def baseSchemaFields(stringColumnsToAdd=[]):\n",
    "    \n",
    "    # clone this w a slice so we dont polute the global, should be a func TODO\n",
    "    fields = BASE_GLOBAL_COLUMNS_SCHEMA[:] \n",
    "    for colName in stringColumnsToAdd:\n",
    "        fields.append(StructField(colName,StringType(),False))\n",
    "    return fields\n",
    "\n",
    "\n",
    "# utility to return schema/mapper handler for the \"adjustment\" records\n",
    "# ie 'padj', 'badj','ladj'\n",
    "def getAdjustmentHandler():\n",
    "    return { \n",
    "        'schema': StructType(baseSchemaFields([\"who\", \"what\"])),\n",
    "        'mapper': lambda rec: getBaseColumns(rec) + getFields(rec)[1:]\n",
    "    }\n",
    "\n",
    "## TODO get consistent with underscore vs camelCase for columns\n",
    "SCHEMA_BY_TYPE = {    \n",
    "    \n",
    "    # sample line: start,howar001,\"Ryan Howard\",0,4,3               \n",
    "    # sample line: sub,waldj001,\"Jordan Walden\",0,0,1\n",
    "    'start': { \n",
    "        'schema': StructType(baseSchemaFields() + [\n",
    "            StructField(\"player_id\",StringType(),False), \n",
    "            StructField(\"playerName\",StringType(),False), \n",
    "            StructField(\"home\",BooleanType(),False), \n",
    "            StructField(\"battingOrder\",ByteType(),False),\n",
    "            StructField(\"position\",ByteType(),False)\n",
    "        ]), \n",
    "        'mapper': lambda rec: getBaseColumns(rec) + [\n",
    "            getFields(rec)[1],\n",
    "            getFields(rec)[2],\n",
    "            (\"0\" == getFields(rec)[3]),  ## TODO verify \"0\" is home\n",
    "            int(getFields(rec)[4]),\n",
    "            int(re.sub('[^0-9]','',getFields(rec)[5])) # make this a reusable proc            \n",
    "        ]\n",
    "    },  \n",
    "    \n",
    "    # sample line: play,6,1,bondb001,02,CFX,HR/9.3-H;2-H;1-H  \n",
    "    'play': { \n",
    "        'schema': StructType(baseSchemaFields() + [ \n",
    "\n",
    "            StructField(\"inning\",ByteType(),False), \n",
    "            StructField(\"topOfInning\",BooleanType(),False), \n",
    "            StructField(\"player_id\",StringType(),False), \n",
    "            StructField(\"count\",ArrayType(ByteType(), False),True), \n",
    "            StructField(\"pitch_seq\",StringType(),False),\n",
    "            StructField(\"description\",StringType(),False)\n",
    "        ]),  \n",
    "        'mapper': lambda rec: getBaseColumns(rec) + [\n",
    "            int(getFields(rec)[1]),             \n",
    "            bool(\"0\" == getFields(rec)[2]),\n",
    "            getFields(rec)[3],\n",
    "            [int(c) for c in list(getFields(rec)[4])] if getFields(rec)[4].isdigit() else [],\n",
    "            getFields(rec)[5],\n",
    "            getFields(rec)[6]            \n",
    "        ]\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    # sample line: \n",
    "    # com,\"$Career homer 587 to pass Frank Robinson for 4th all-time\"\n",
    "    \n",
    "    # or sample multi-line comment:\n",
    "    # com,\"$Hall caught in rundown while Winn advanced to 3B; both players\"\n",
    "    # com,\"ended up on 3B and Winn is tagged out; Hall thought he was the one\"\n",
    "    # com,\"who was out and stepped off the bag and is tagged out\"  \n",
    "    ## TODO collapse all of these com records should be one comment\n",
    "    'com': { \n",
    "        'schema': StructType(baseSchemaFields([\"comment\"])),\n",
    "        'mapper': lambda rec: getBaseColumns(rec) + [\n",
    "            getFields(rec)[1]\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \n",
    "    # sample line: info,attendance,41128\n",
    "    # most have the type a key and value, but there are multiple values\n",
    "    # so: info,scorer,96,387,269,107,80,104,163,274,395\n",
    "    \n",
    "   'info': { \n",
    "        'schema': StructType(\n",
    "            baseSchemaFields([\"key\"]) + [StructField(\"values\",ArrayType(StringType()),False)]\n",
    "        ),\n",
    "        'mapper': multiValueRecordMapper\n",
    "    },\n",
    "    \n",
    "    # sample line: data,er,fyhrm001,0\n",
    "\n",
    "    'data': { \n",
    "        'schema': StructType(baseSchemaFields([\"type\", \"key\", \"value\"])),\n",
    "        'mapper': lambda rec: getBaseColumns(rec) + getFields(rec)[1:]\n",
    "    },       \n",
    "\n",
    "    # lastly, the adjustment records\n",
    "    # \n",
    "    # sample line: padj,harrg001,L\n",
    "    # sample line: ladj,0,9\n",
    "    # sample line: badj,bonib001,R    \n",
    "    \n",
    "    'padj': getAdjustmentHandler(),       \n",
    "    'badj': getAdjustmentHandler(),       \n",
    "    'ladj': getAdjustmentHandler(),       \n",
    "}\n",
    "\n",
    "# \"sub\" records are the same as \"start\" records\n",
    "SCHEMA_BY_TYPE['sub'] = SCHEMA_BY_TYPE['start']\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfByType = {}\n",
    "\n",
    "# for recType,typeInfo in {col: SCHEMA_BY_TYPE[col] for col in ['info']}.items():\n",
    "for recType,typeInfo in SCHEMA_BY_TYPE.items():    \n",
    "    print(\"processing %s ...\" % (recType))\n",
    "\n",
    "    curRdd = combinedFlattenedEvents.filter(lambda rec: rec[-1][0]==recType)\n",
    "    if curRdd.isEmpty(): continue \n",
    "    dfByType[recType] = sqlContext.createDataFrame(\n",
    "        curRdd.map(typeInfo['mapper']), typeInfo['schema']\n",
    "    )\n",
    "\n",
    "    dfByType[recType].registerTempTable(recType)\n",
    "    print(\"%s has %d records\" % (recType,dfByType[recType].count()))\n",
    "          \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following utils for both the roster files, ie *.ROS, and team files \n",
    "# TEAMyyyy. it prepends the rows in the file with its filename to extract\n",
    "# things like the year\n",
    "\n",
    "\n",
    "## this proc does two things, first it is a map function for a wholeTextFiles() call. It is\n",
    "## meant to be called with a flatmap because it returns the rows of the file with the last part\n",
    "## of the filename preprended to the records from the file. Additionally, this method can do a\n",
    "## transforms on the file part which is passed in with the optional pathTransform parameter. this\n",
    "## defaults to just an identity function but can be used to trim the path and cast to an int, say.\n",
    "def getWholeTextParserWithPathTransform(pathTransform=lambda x:x, targetRecords=None):\n",
    "    def wholeTextParserWithPath( record ): \n",
    "        (path, content) = record\n",
    "        fileName = splitext(basename(path))[0]\n",
    "        return [ \n",
    "            [pathTransform(fileName)] + record.split(\",\") \n",
    "            for record in content.splitlines() if (\n",
    "                    targetRecords is None or len(record.split(\",\")) == targetRecords\n",
    "                )\n",
    "        ]\n",
    "    return wholeTextParserWithPath\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ROSTER files\n",
    "ROSTER = \"roster\"\n",
    "\n",
    "# sample greiz001,Greinke,Zack,R,R,LAN,P\n",
    "rosterSchema = StructType([\n",
    "    # prepended from filename\n",
    "    StructField(\"year\",ShortType(),False), \n",
    "\n",
    "    # rest from record\n",
    "    StructField(\"player_id\",StringType(),False), \n",
    "    StructField(\"firstName\",StringType(),False), \n",
    "    StructField(\"lastName\",StringType(),False), \n",
    "    StructField(\"bats\",StringType(),False), \n",
    "    StructField(\"throws\",StringType(),False), \n",
    "    StructField(\"team\",StringType(),False), \n",
    "    StructField(\"position\",StringType(),False)\n",
    "])\n",
    "\n",
    "rosterRowsRDD = sc.wholeTextFiles(rosterPath).flatMap(\n",
    "    getWholeTextParserWithPathTransform(lambda x:int(x[3:])) # trim year part and cast to int\n",
    ")\n",
    "rosterDataFrame = sqlContext.createDataFrame(rosterRowsRDD,rosterSchema)\n",
    "rosterDataFrame.registerTempTable(ROSTER)\n",
    "print(\"%s has %d rows\" % (ROSTER,rosterDataFrame.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TEAM files\n",
    "TEAM = \"team\"\n",
    "\n",
    "# sample MIL,N,Milwaukee,Brewers\n",
    "teamSchema = StructType([\n",
    "    # prepended from filename\n",
    "    StructField(\"year\",ShortType(),False), \n",
    "\n",
    "    # rest from record\n",
    "    StructField(\"team_id\",StringType(),False), \n",
    "    StructField(\"league\",StringType(),False), \n",
    "    StructField(\"city\",StringType(),False), \n",
    "    StructField(\"teamName\",StringType(),False)\n",
    "])\n",
    "\n",
    "teamRowsRDD = sc.wholeTextFiles(teamPath).flatMap(\n",
    "    getWholeTextParserWithPathTransform(lambda x:int(x[4:]),targetRecords=4)    \n",
    ")\n",
    "teamDataFrame = sqlContext.createDataFrame(teamRowsRDD, teamSchema)\n",
    "teamDataFrame.registerTempTable(TEAM)\n",
    "print(\"%s has %d rows\" % (TEAM, teamDataFrame.count()))\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tableName in sqlContext.tableNames():\n",
    "    path = DIST_PARQUET_DIR + tableName\n",
    "    print(\"saving %s to %s\" % (tableName, path))\n",
    "    sqlContext.table(tableName).write.save(path=path, format=\"parquet\", mode=\"overwrite\")\n",
    "\n",
    "print(\"done\")    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
